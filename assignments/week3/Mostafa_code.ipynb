{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d27e4681-b8da-41ff-b212-bd21e9c02735",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Import and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4f44235-0f1a-4605-8db4-c4de038a0f5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run the MLP training and evaluation pipeline.\n",
    "\"\"\"\n",
    "\n",
    "from model_factory import create_model\n",
    "\n",
    "# MNIST:\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Compose\n",
    "\n",
    "# PyTorch:\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "# Other:\n",
    "from typing import Tuple , Callable\n",
    "from tqdm import tqdm\n",
    "\n",
    "# The transform list is a set of operations that we apply to the data\n",
    "# before we use it. In this case, we convert the data to a tensor and\n",
    "# flatten it. (Thought-exercise: Why do we need to flatten the data?)\n",
    "_transform_list = [\n",
    "    ToTensor(),\n",
    "    lambda x: x.view(-1),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462c693e-fa53-4610-9627-04c57cd00fcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_mnist_data() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Get the MNIST data from torchvision.\n",
    "\n",
    "    Arguments:\n",
    "        None\n",
    "\n",
    "    Returns:\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        test_loader (DataLoader): The test data loader.\n",
    "\n",
    "    \"\"\"\n",
    "    # Get the training data:\n",
    "    train_data = MNIST(\n",
    "        root=\"data\", train=True, download=True, transform=Compose(_transform_list)\n",
    "    )\n",
    "    # Create a data loader for the training data:\n",
    "    train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "    # Get the test data:\n",
    "    test_data = MNIST(\n",
    "        root=\"data\", train=False, download=True, transform=Compose(_transform_list)\n",
    "    )\n",
    "    # Create a data loader for the test data:\n",
    "    test_loader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
    "    # Return the data loaders:\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    learning_rate: float,\n",
    "    device: torch.device,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train a model on the MNIST data.\n",
    "\n",
    "    Arguments:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        test_loader (DataLoader): The test data loader.\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        learning_rate (float): The learning rate to use.\n",
    "        device (torch.device): The device to use for training.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    \"\"\"\n",
    "    # Create an optimizer:\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    # Create a loss function:\n",
    "    criterion = CrossEntropyLoss()\n",
    "    # Move the model to the device:\n",
    "    model.to(device)\n",
    "    # Create a progress bar:\n",
    "    progress_bar = tqdm(range(num_epochs))\n",
    "    # Train the model:\n",
    "    for epoch in progress_bar:\n",
    "        # Set the model to training mode:\n",
    "        model.train()\n",
    "        # Iterate over the training data:\n",
    "        for batch in train_loader:\n",
    "            # Get the data and labels:\n",
    "            data, labels = batch\n",
    "            # Move the data and labels to the device:\n",
    "            data = data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Zero the gradients:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass:\n",
    "            outputs = model(data)\n",
    "            # Calculate the loss:\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward pass:\n",
    "            loss.backward()\n",
    "            # Update the parameters:\n",
    "            optimizer.step()\n",
    "        # Set the model to evaluation mode:\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate the accuracy on the test data:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                # Get the data and labels:\n",
    "                data, labels = batch\n",
    "                # Move the data and labels to the device:\n",
    "                data = data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                # Forward pass:\n",
    "                outputs = model(data)\n",
    "                # Get the predictions:\n",
    "                _, predictions = torch.max(outputs.data, 1)\n",
    "                # Update the total and correct counts:\n",
    "                total += labels.size(0)\n",
    "                correct += (predictions == labels).sum().item()\n",
    "        # Calculate the accuracy:\n",
    "        accuracy = correct / total\n",
    "        # Update the progress bar:\n",
    "        progress_bar.set_description(f\"Epoch: {epoch}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8479353f-575b-4d8f-8771-d205c5f36928",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a28ab494-1a37-4e0c-a3d0-c5e49c48a206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fef593a9550>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3c08602-74cd-44c4-808c-d0dbfc751279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1658e6b3-8981-4ef9-94a4-88aa48717a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, test_loader = get_mnist_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "972bb185-7f07-44df-9aa4-1d53356c6ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim: int, output_dim: int) -> MLP:\n",
    "    \"\"\"\n",
    "    Create a multi-layer perceptron model.\n",
    "\n",
    "    Arguments:\n",
    "        input_dim (int): The dimension of the input data.\n",
    "        output_dim (int): The dimension of the output data.\n",
    "        hidden_dims (list): The dimensions of the hidden layers.\n",
    "\n",
    "    Returns:\n",
    "        MLP: The created model.\n",
    "\n",
    "    \"\"\"\n",
    "    return MLP(input_dim, 128, output_dim, 3, torch.nn.ReLU, torch.nn.init.xavier_uniform_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6de5da7f-aece-42ec-9cfe-e0d1f4b7f779",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(784, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "45d73f2a-7c60-44ce-8b67-f8b9e63bb69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Accuracy: 0.9770: 100%|███████████████| 10/10 [01:13<00:00,  7.33s/it]\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=10,\n",
    "        learning_rate=0.001,\n",
    "        device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6aecd516-c329-4a2d-9da0-b9d02e8561f4",
   "metadata": {},
   "source": [
    "Neurons per layer:\n",
    "    MLP(input_dim, 16, output_dim, 2, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.9518\n",
    "    MLP(input_dim, 32, output_dim, 2, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.9652\n",
    "    MLP(input_dim, 40, output_dim, 2, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.9716\n",
    "    MLP(input_dim, 64, output_dim, 2, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.9734\n",
    "    MLP(input_dim, 128, output_dim, 2, torch.nn.ReLU, torch.nn.init.xavier_uniform_)-> 0.9778"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6836091-040a-47be-9977-0d5cf2175858",
   "metadata": {},
   "source": [
    "Number of hidden layers:\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.969\n",
    "    MLP(input_dim, 32, output_dim, 2, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.9722\n",
    "    MLP(input_dim, 32, output_dim, 3, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.968\n",
    "    MLP(input_dim, 32, output_dim, 4, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.968"
   ]
  },
  {
   "cell_type": "raw",
   "id": "59d4f6ea-90fc-4681-b633-d7b57cf42349",
   "metadata": {},
   "source": [
    "Activation functions:\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.96\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ELU, torch.nn.init.xavier_uniform_)  -> 0.96\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.Hardshrink, torch.nn.init.xavier_uniform_) -> 0.92\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.Hardsigmoid, torch.nn.init.xavier_uniform_)-> 0.95\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.LogSigmoid, torch.nn.init.xavier_uniform_) -> 0.96\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.Tanh, torch.nn.init.xavier_uniform_)       -> 0.96"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e34fad09-1cbc-47cb-8945-7b090dda1606",
   "metadata": {},
   "source": [
    "Initializations:\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.ones_)           -> 0.91/92\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.xavier_uniform_) -> 0.96\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.normal_)         -> 0.90\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.eye_)            -> 0.9526\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.xavier_normal_)  -> 0.96\n",
    "    MLP(input_dim, 32, output_dim, 1, torch.nn.ReLU, torch.nn.init.kaiming_uniform_)-> 0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f560871-1615-44c7-a5f8-88ca55e12a20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi-env",
   "language": "python",
   "name": "mi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
