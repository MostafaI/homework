{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fc3853c-8597-456a-9994-469007e842e2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "b2ee2685-12c2-47e2-a00f-fb0b596c2a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A complete implementation and training of a CIFAR10 classifier.\n",
    "The prompt is to create another LearningRateScheduler.\n",
    "\"\"\"\n",
    "import time\n",
    "from typing import Tuple, Callable\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from tqdm import tqdm\n",
    "\n",
    "from model import Model\n",
    "from config import CONFIG\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "a84fa54b-f33d-412e-b242-a200e2cb246f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_cifar10_data() -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Get the CIFAR10 data from torchvision.\n",
    "    Arguments:\n",
    "        None\n",
    "    Returns:\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        test_loader (DataLoader): The test data loader.\n",
    "    \"\"\"\n",
    "    # Get the training data:\n",
    "    train_data = CIFAR10(\n",
    "        root=\"data/cifar10\", train=True, download=True, transform=CONFIG.transforms\n",
    "    )\n",
    "    # Create a data loader for the training data:\n",
    "    train_loader = DataLoader(train_data, batch_size=CONFIG.batch_size, shuffle=True)\n",
    "    # Get the test data:\n",
    "    test_data = CIFAR10(\n",
    "        root=\"data/cifar10\", train=False, download=True, transform=CONFIG.transforms\n",
    "    )\n",
    "    # Create a data loader for the test data:\n",
    "    test_loader = DataLoader(test_data, batch_size=CONFIG.batch_size, shuffle=True)\n",
    "    # Return the data loaders:\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def train(\n",
    "    model: torch.nn.Module,\n",
    "    train_loader: DataLoader,\n",
    "    test_loader: DataLoader,\n",
    "    num_epochs: int,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    criterion: torch.nn.Module,\n",
    "    device: torch.device = device,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Train a model on the data.\n",
    "    Arguments:\n",
    "        model (torch.nn.Module): The model to train.\n",
    "        train_loader (DataLoader): The training data loader.\n",
    "        test_loader (DataLoader): The test data loader.\n",
    "        num_epochs (int): The number of epochs to train for.\n",
    "        optimizer (torch.optim.Optimizer): The optimizer to use.\n",
    "        criterion (torch.nn.Module): The loss function to use.\n",
    "        learning_rate_scheduler (torch.optim.lr_scheduler._LRScheduler): The\n",
    "            learning rate scheduler to use.\n",
    "        device (torch.device): The device to use for training.\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Move the model to the device:\n",
    "    model.to(device)\n",
    "    # Loop over the epochs:\n",
    "    for epoch in range(num_epochs):\n",
    "        # Set the model to training mode:\n",
    "        model.train()\n",
    "        # Loop over the training data:\n",
    "        for x, y in tqdm(train_loader):\n",
    "            # Move the data to the device:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            # Zero the gradients:\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass:\n",
    "            y_hat = model(x)\n",
    "            # Compute the loss:\n",
    "            loss = criterion(y_hat, y)\n",
    "            # Backward pass:\n",
    "            loss.backward()\n",
    "            # Update the parameters:\n",
    "            optimizer.step()\n",
    "        # Set the model to evaluation mode:\n",
    "        model.eval()\n",
    "        # Compute the accuracy on the test data:\n",
    "        accuracy = compute_accuracy(model, test_loader, device)\n",
    "        if accuracy > ACCURACY_THRESHOLD:\n",
    "            break\n",
    "        # Print the results:\n",
    "        print(f\"Epoch {epoch + 1} | Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "\n",
    "def compute_accuracy(\n",
    "    model: torch.nn.Module, data_loader: DataLoader, device: torch.device = device\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the accuracy of a model on some data.\n",
    "    Arguments:\n",
    "        model (torch.nn.Module): The model to compute the accuracy of.\n",
    "        data_loader (DataLoader): The data loader to use.\n",
    "        device (torch.device): The device to use for training.\n",
    "    Returns:\n",
    "        accuracy (float): The accuracy of the model on the data.\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode:\n",
    "    model.eval()\n",
    "    # Initialize the number of correct predictions:\n",
    "    num_correct = 0\n",
    "    # Loop over the data:\n",
    "    for x, y in data_loader:\n",
    "        # Move the data to the device:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        # Forward pass:\n",
    "        y_hat = model(x)\n",
    "        # Compute the predictions:\n",
    "        predictions = torch.argmax(y_hat, dim=1)\n",
    "        # Update the number of correct predictions:\n",
    "        num_correct += torch.sum(predictions == y).item()\n",
    "    # Compute the accuracy:\n",
    "    accuracy = num_correct / len(data_loader.dataset)\n",
    "    # Return the accuracy\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f67fc3-3d50-4150-b55b-84e0f72c6146",
   "metadata": {},
   "source": [
    "## Code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "848e6ae5-893c-4e6b-b753-2cb6dbaae1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACCURACY_THRESHOLD = 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "bfd92f96-138f-4e44-b23b-a737c7aa72f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = get_cifar10_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "f42eddf6-2d8e-4e17-9fb5-b1f7277a73e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    batch_size = 64*4\n",
    "    num_epochs = 2\n",
    "\n",
    "    optimizer_factory: Callable[\n",
    "        [nn.Module], torch.optim.Optimizer\n",
    "    ] = lambda model: torch.optim.Adam(model.parameters(), lr=3e-3)\n",
    "\n",
    "    transforms = Compose([ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "8ba252ca-ebaa-4674-8825-4ba492b575c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, num_channels: int, num_classes: int) -> None:\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=num_channels, out_channels=16, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3)\n",
    "        # self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(576, 128)\n",
    "        self.fc2 = nn.Linear(128,num_classes)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        computes the output of the model\n",
    "        \"\"\"\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool(x)\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        # x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1) # Flatten\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(torch.relu(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "0f032062-d1b8-47b1-a377-270afa316a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model:\n",
    "model = Model(num_channels=3, num_classes=10)\n",
    "# Create the optimizer:\n",
    "optimizer = CONFIG.optimizer_factory(model)\n",
    "# Create the loss function:\n",
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "f1c84aed-90b2-44e6-94b0-09cf36f4b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                  | 0/782 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x576 and 288x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [412]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m tic \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m toc \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[0;32mIn [353]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, test_loader, num_epochs, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     61\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Forward pass:\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Compute the loss:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_hat, y)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mi-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [410]\u001b[0m, in \u001b[0;36mModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Flatten\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(torch\u001b[38;5;241m.\u001b[39mrelu(x))\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mi-env/lib/python3.10/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/mi-env/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (64x576 and 288x128)"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    num_epochs=CONFIG.num_epochs,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    ")\n",
    "toc = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "25909142-6659-46bc-a901-00d52f51b846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 37.74 seconds, final accuracy: 0.56\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Training time: {toc - tic:.2f} seconds, final accuracy: {compute_accuracy(model, test_loader):.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1be9ef9-b1bc-4109-bce4-96c143d3116f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B64, C16-bn-pool, c32-bn-pool, \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7bdfdb8b-ae6a-4583-8c5c-406f0855cd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# B32, C1-16-k3, C2-32-k3, 37s\n",
    "# B32, C1-32-k3, C2-32-k3, 95s\n",
    "# B64, C1-32-k3, C2-32-k3, 45s\n",
    "# LR-1 -> B64, C1-32-k3, C2-32-k3 -> 47\n",
    "# LR-15 -> B64, C1-32-k3, C2-32-k3 -> 46\n",
    "\n",
    "\n",
    "\n",
    "# B32, C1-16-k5, C2-32-k3, 67.41 - 2batches\n",
    "# B32, C1-32-k5, C2-32-k3, 87.68\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b0bfd1-7383-430c-9dbd-b74caa108d17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mi-env",
   "language": "python",
   "name": "mi-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
